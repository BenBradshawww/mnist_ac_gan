{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import os \n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Dense, Reshape, BatchNormalization, ReLU, Concatenate, Activation, LeakyReLU, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    latent_dim=100\n",
    "    num_classes=10\n",
    "    noise_dim=100\n",
    "    batch_size=32\n",
    "    train_size=40000\n",
    "    num_batches=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = tfds.load('mnist', split=['train[:80%]', 'train[80%:]', 'test'], as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(features, labels):\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "\n",
    "    features = (features / 127.5) - 1\n",
    "\n",
    "    labels = tf.keras.utils.to_categorical(labels, CFG.num_classes)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.map(preprocessing, num_parallel_calls=AUTO)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000)\n",
    "train_ds = train_ds.batch(batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image file\n",
    "Image(filename='example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    noise = Input(shape=(CFG.noise_dim,))\n",
    "    label = Input(shape=(CFG.num_classes,))\n",
    "\n",
    "    inputs = Concatenate()([noise, label])\n",
    "\n",
    "    x = Dense(units=128*7*7, activation='relu')(inputs)\n",
    "    x = Reshape(target_shape=(7,7,128))(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', use_bias=False)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', use_bias=False)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=3, padding='same')(x)\n",
    "    outputs = Activation(activation='tanh')(x)\n",
    "\n",
    "    model = Model(inputs=[noise, label], outputs=outputs, name='AC-GAN-Gen')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    \n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "\n",
    "    flatten = Flatten()(x)\n",
    "\n",
    "    validity = Dense(units=1, activation='sigmoid', name='valid_dense')(flatten)\n",
    "    label = Dense(units=CFG.num_classes, activation='softmax', name='label_dense')(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[validity, label], name='AC-Gan-Disc')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    real_loss = cross_entropy(tf.zeros_like(real_output), real_output)\n",
    "    total_loss = fake_loss + real_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "# Loss functions\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "sparse_categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, labels, pred):\n",
    "    real_loss = binary_cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = binary_cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    classifier_loss = sparse_categorical_cross_entropy(labels, pred)\n",
    "\n",
    "    total_loss = real_loss + fake_loss + classifier_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output, fake_labels, fake_pred):\n",
    "\n",
    "    fake_loss = binary_cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    classifier_loss = sparse_categorical_cross_entropy(fake_labels, fake_pred)\n",
    "\n",
    "    total_loss = fake_loss + classifier_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(4, 4))\n",
    "    axes = axes.flatten() \n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        axes[i].imshow(predictions[i, :, :, 0] * 127.5, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title('Class: '+str(i%10), fontsize=8)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        plt.savefig('./training_images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(gen_loss, disc_loss):\n",
    "\n",
    "\t# Adding loss to train loss file\n",
    "\tif os.path.isfile('train_loss.csv'):\n",
    "\t\tdf = pd.read_csv('train_loss.csv')\n",
    "\telse:\n",
    "\t\tdf = pd.DataFrame(columns=['gen_loss', 'disc_loss'])\n",
    "\tprint('Gen Loss:', gen_loss, 'Discriminator Loss', disc_loss)\n",
    "\tnew_row = {'gen_loss':gen_loss, 'disc_loss':disc_loss}\n",
    "\tdf.loc[len(df)] = new_row\n",
    "\t\n",
    "\tplt.plot(df.index, df['gen_loss'], label='gen loss', color='blue')\n",
    "\tplt.plot(df.index, df['disc_loss'], label='disc loss', color='red')\n",
    "\tplt.title('Training Loss')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\t\n",
    "\tdf.to_csv('train_loss.csv', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, label):\n",
    "    noise = tf.random.normal([CFG.batch_size, CFG.noise_dim])\n",
    "    noise_label_ = tf.random.uniform([CFG.batch_size],\n",
    "                                        minval=0,\n",
    "                                        maxval=9,\n",
    "                                        dtype=tf.dtypes.int64)\n",
    "                                        \n",
    "    noise_label = tf.one_hot(noise_label_,10)\n",
    "    seed = [noise,noise_label]\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        gen_images = generator(seed, training=False)\n",
    "\n",
    "        real_output, real_pred = discriminator(image, training=True)\n",
    "        fake_output, fake_pred = discriminator(gen_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output, noise_label, fake_pred)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, label, fake_pred)        \n",
    "\n",
    "    gen_grad = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
    "    disc_grad = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(disc_grad, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './acgan_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_image = tf.random.normal([16, CFG.noise_dim])\n",
    "seed_label = tf.one_hot([i%10 for i in range(16)], 10)\n",
    "seed = [seed_image, seed_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, checkpoint_restore):\n",
    "\n",
    "\tif checkpoint_restore:\n",
    "\t\tcheckpoint_weights = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\tif checkpoint_weights:\n",
    "\t\t\tcheckpoint.restore(checkpoint_weights)\n",
    "\t\t\tprint(\"Checkpoint restored from\", checkpoint_weights)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"No checkpoint found.\")\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tstart = time.time()\n",
    "\t\tfor image, label in tqdm(dataset):\n",
    "\t\t\tgen_loss, disc_loss = train_step(image, label)\n",
    "\t\t\tgen_loss, disc_loss = gen_loss.numpy(), disc_loss.numpy()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tdisplay.clear_output(wait=True)\n",
    "\t\tgenerate_and_save_images(generator,epoch+1,seed)\n",
    "\t\tplot_loss(gen_loss=gen_loss, disc_loss=disc_loss)\n",
    "\n",
    "\t\tif epoch % 10 == 0:\n",
    "\t\t\tcheckpoint.save(file_prefix=checkpoint_prefix)\n",
    "\t\t\tprint(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\t\t\n",
    "\t\tprint('Time for epoch {} is {} sec'.format(epoch+1,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_ds, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
